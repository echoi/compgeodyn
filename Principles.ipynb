{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principles of Numerical Mathematics\n",
    "\n",
    "From Ch. 2 of Quarteroni (2000) and https://github.com/mitmath/18335/blob/master/notes/Floating-Point-Intro.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Definition of problem types\n",
    "A problem is to find $x$ suth that $F(x,d)=0$, where $d$ is a set of data and $F$ is the functional relationship between $x$ and $d$.\n",
    "- Direct  : $x$ unknown, $F$ and $d$ given\n",
    "- Inverse : $d$ unknown, $F$ and $x$ given\n",
    "- Identification : $F$ unknown, $x$ and $d$ given\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "A (PDE) problem is called *well posed* or *stable* provided it has a **unique** solution which depends **continuously** on the given data. Otherwise, it is called *ill-posed* or *improperly posed* or *unstable.*\n",
    "#### Example\n",
    "\\begin{equation}\n",
    "p(x) = x^{4}-x^{2}(2a-1)+a(a-1) \n",
    "\\end{equation}\n",
    "has discontinuous variation of the number of real roots: 4 if $a \\ge 1$, 2 if $a \\in [0,1)$ and 0 if $a \\lt 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "\n",
    "#### Meaning of *continuous dependence* \n",
    "Continuous dependence on the data means that **small** perturbations on the data $d$ yield **small** changes in the solution $x$.\n",
    "- $\\delta d$: An admissiable perturbation on the data\n",
    "- $\\delta x$: The consequent change in the solution\n",
    "- $F(x+\\delta x, d + \\delta d) = 0$, then,\n",
    "\\begin{equation}\n",
    "^{\\forall}\\eta > 0, \\  ^{\\exists}K(\\eta,d) \\text{ such that } \\Vert \\delta d \\Vert < \\eta \\Rightarrow \\Vert \\delta x \\Vert \\le K(\\eta, d) \\Vert \\delta d \\Vert.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Well-posedness and condition number\n",
    "\n",
    "### Well-posed problem\n",
    "\n",
    "#### Condition number\n",
    "- Relative condition number\n",
    "\\begin{equation}\n",
    " K(d) = \\sup_{\\delta d \\in D} \\frac{ \\Vert \\delta x \\Vert / \\Vert x \\Vert}{\\Vert \\delta d \\Vert / \\Vert d \\Vert},\n",
    "\\end{equation}\n",
    "where $D$ is a small neighborhood around the origin for *admissable* perturbations.\n",
    "- Absolute condition number for $x=0$ or $d=0$\n",
    "\\begin{equation}\n",
    " K(d) = \\sup_{\\delta d \\in D} \\frac{\\Vert \\delta x \\Vert}{\\Vert \\delta d \\Vert}.\n",
    "\\end{equation}\n",
    "\n",
    "- *ill-conditioned* if $K(d)$ is *big*, of which precise meaning varies from one problem to another.\n",
    "- *ill-conditioned* $\\neq$ *ill-posed*! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example 1\n",
    "\\begin{equation}\n",
    "x^{2} - 2px + 1 = 0 \\quad (p \\ge 1)\n",
    "\\end{equation}\n",
    "The solution is $x_{\\pm}=p \\pm \\sqrt{p^{2}-1}$. \n",
    "Now, let a function $F(x,p) = x^{2}-2px +1$, where the \"datum\" is the coefficient $p$, and $x$ is the unknown vector of components $\\{x_{+},x_{-}\\}$.\n",
    "\n",
    "Let's also define a *resolvent* $G: x=G(p)$, where $x$ is the solution, such that $F(G(p),p)=0$.\n",
    "Expressing the solutions using resolvents, we get\n",
    "\\begin{equation}\n",
    "  x_{\\pm} = G_{\\pm}(p) = p \\pm \\sqrt{p^{2}-1}.\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    "  G^{\\prime}_{\\pm}(p) = 1 \\pm \\frac{p}{\\sqrt{p^{2}-1}}.\n",
    "\\end{equation}\n",
    "\n",
    "From\n",
    "\\begin{equation}\n",
    "x + \\delta x = G(p+\\delta p),\n",
    "\\end{equation}\n",
    "we get\n",
    "\\begin{equation}\n",
    "\\delta x = G(p+\\delta p)-x = G(p+\\delta p)-G(p).\n",
    "\\end{equation}\n",
    "\n",
    "The Taylor's expansion of $G$ tells us that\n",
    "\\begin{equation}\n",
    " G(p+\\delta p)-G(p) = G^{\\prime}(p)\\delta p + O(||\\delta p||) \\quad \\text{for} \\quad \\delta p \\rightarrow 0.\n",
    "\\end{equation}\n",
    "\n",
    "Thus, \n",
    "\\begin{equation}\n",
    "\\delta x \\approx G^{\\prime}(p)\\delta p.\n",
    "\\end{equation}\n",
    "\n",
    "Now we get the following form of the condition number:\n",
    "\\begin{equation}\n",
    "K(p) = \\sup_{\\delta p \\in D} \\frac{\\Vert \\delta x \\Vert / \\Vert x \\Vert}{\\Vert \\delta p \\Vert / \\Vert p \\Vert } = \\sup_{\\delta p \\in D} \\frac{ \\Vert G^{\\prime}(p)\\delta p \\Vert  \\,  \\Vert p \\Vert }{ \\Vert G(p) \\Vert  \\,  \\Vert \\delta p \\Vert }\n",
    " \\approx  \\Vert G^{\\prime}(p) \\Vert  \\frac{ \\Vert p \\Vert }{ \\Vert G(p) \\Vert }\n",
    "\\end{equation}\n",
    "\n",
    "Finally, we get\n",
    "\\begin{equation}\n",
    "K(p) = \\left\\Vert 1 \\pm \\frac{p}{\\sqrt{p^{2}-1}}  \\right\\Vert \\frac{\\Vert p\\Vert }{\\Vert p \\pm \\sqrt{p^{2}-1} \\Vert} = \\frac{|p|}{\\sqrt{p^{2}-1}}\n",
    "\\end{equation}\n",
    "\n",
    "So, $K$ is small (.ie., $\\sim$1) for $p \\ge \\sqrt{2}$; goes to $\\infty$ as $p \\rightarrow 1$ making the problem (ill-posed or ill-conditioned?).\n",
    "\n",
    "However, this problem can **regularized**: i.e., the singularity at $p=1$ can be removed by the change of parameters.\n",
    "By letting $t = p + \\sqrt{p^{2}-1}$ and $F(x,t) = x^{2} - ((1+t^{2})/t)x + 1 = 0$, the roots become $x=t$ and $1/t$ and coincide with each other when $t=1$ (i.e., $p=1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x( p ):\n",
    "    return np.array([p+np.sqrt(p**2-1),p-np.sqrt(p**2-1)])\n",
    "\n",
    "eps1 = 1e-6\n",
    "p1 = 1.0 + eps1\n",
    "K1 = p1 / (p1**2 - 1)\n",
    "x1 = np.abs(get_x(p1)[0])\n",
    "dp1 = np.random.rand(10000)*eps1\n",
    "x1vals = np.abs( get_x( p1 + dp1 )[0] - x1 ) / x1\n",
    "\n",
    "p2 = 1e4\n",
    "K2 = p2 / np.sqrt(p2**2 - 1)\n",
    "x2 = np.abs(get_x(p2)[0])\n",
    "dp2 = np.random.rand(10000)*1e2\n",
    "x2vals = np.abs( get_x( p2 + dp2 )[0] - x2 ) / x2\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(8,4))\n",
    "axs[0].plot(dp1/p1, x1vals)\n",
    "axs[0].set_xlabel(\"$\\Vert \\delta p \\Vert/\\Vert p \\Vert$\", fontsize=12)\n",
    "axs[0].set_ylabel(\"$\\Vert \\delta x \\Vert/\\Vert x \\Vert$\", fontsize=12)\n",
    "axs[0].text(1.0e-7,0.0005,\"$K = ${K:e}\".format(K=K1))\n",
    "axs[0].set_title(\"$p \\sim 1$\", fontsize=14)\n",
    "axs[1].plot(dp2/p2, x2vals)\n",
    "axs[1].set_xlabel(\"$\\Vert \\delta p \\Vert/\\Vert p \\Vert$\", fontsize=12)\n",
    "axs[1].set_ylabel(\"$\\Vert \\delta x \\Vert/\\Vert x \\Vert$\", fontsize=12)\n",
    "axs[1].text(1.0e-3,0.008,\"$K = ${K:e}\".format(K=K2))\n",
    "axs[1].set_title(\"$ p \\sim 10^{4}$\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Example 2\n",
    "Let's consider a linear mapping:\n",
    "\\begin{equation}\n",
    " f: \\mathbb{R}^{2} \\rightarrow \\mathbb{R}, \\quad f(a,b) = a+b\n",
    "\\end{equation}\n",
    "\n",
    "Let's note that the resolvent $G$ is equal to $f(a,b)$ in this case and the gradient is the vector $f^{\\prime}(a,b)=(1,1)$. Also, the data ($d$) in this problem is a vector composed of two parameters, $(a,b)$.\n",
    "Using $L_{1}$ norm ($\\Vert \\mathbf{a} \\Vert_{1}=\\sum_{i=1}^{n}|a_{i}|$), we get the condition number:\n",
    "\\begin{equation}\n",
    "  K(a,b) \\approx \\Vert G^{\\prime}(d) \\Vert_{1}  \\frac{ \\Vert d \\Vert_{1} }{ \\Vert G(d) \\Vert_{1} } = 2\\frac{|a|+|b|}{|a+b|}\n",
    "\\end{equation}\n",
    "\n",
    "If $a$, $b$ are of the same sign, $K=1$ and the problem is will-posed;\n",
    "if $a$, $b$ are almost equal but of the opposite signs, $K \\rightarrow \\infty$ and the problem is ill-conditioned.\n",
    "The ill-conditioned situation arises from the **cancellation of significant digits**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "A numerical method for the approximate solution of $F(x,d)=0$ will consist, in general, of a sequence of approximate problems\n",
    "\\begin{equation}\n",
    "F_{n}(x_{n}, d_{n})=0 \\quad n \\ge 1\n",
    "\\end{equation}\n",
    "such that $x_{n} \\rightarrow x$ as $n \\rightarrow \\infty$: In other words, **the numerical solution converges to the exact solution**. A typical example of data $d_{n}$ is grid spacing (often denoted as $h$) that are sequentially refined. \n",
    "\n",
    "In methods for finding an approximate solution to a partial differential equation such as finite difference method or finite element method, it is a common practice to prove convergence by showing that error or residual decreases at the expected rate as $h$ gets smaller.\n",
    "\n",
    "[//]: # \"Is such a demonstration sufficient to show that the employed numerical method is convergent? Most of the time but not always. That's why we distinguish convergence from **consistency**:\n",
    "The problem sequence is **consistent** if $F_{n}(x,d) - F(x,d) \\rightarrow 0$ for $n \\rightarrow \\infty$.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "For a numerical method to be **stable** or **well-posed**, we require for any fixed $n$,\n",
    "\n",
    "1. there exists a unique solution $x_{n}$ corresponding to $d_{n}$\n",
    "2. $x_{n}$ is a unique and continuous function of $d_{n}$:\n",
    "\\begin{equation}\n",
    "^{\\forall}\\eta > 0, ^{\\exists}K_{n}(\\eta, d_{n}): \\Vert \\delta d_{n} \\Vert < \\eta \\Rightarrow \\Vert \\delta x_{n} \\Vert \\le K_{n} \\Vert \\delta d_{n} \\Vert,\n",
    "\\end{equation}\n",
    "where the condition number $K_{n}$ should be understood as a relative or an absolute one according to the context. Recall the following definitions:\n",
    "\\begin{equation}\n",
    " K_{n}(d_{n}) = \\sup_{\\delta d_{n} \\in D} \\frac{ \\Vert \\delta x_{n} \\Vert / \\Vert x_{n} \\Vert}{\\Vert \\delta d_{n} \\Vert / \\Vert d_{n} \\Vert},\n",
    "\\end{equation}\n",
    "and \n",
    "\\begin{equation}\n",
    " K_{abs,n}(d_{n}) = \\sup_{\\delta d_{n} \\in D} \\frac{\\Vert \\delta x_{n} \\Vert}{\\Vert \\delta d_{n} \\Vert}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Asymptotic condition numbers are defined as follows:\n",
    "\\begin{align}\n",
    "K^{num}(d_{n}) &= \\lim_{k\\rightarrow \\infty} \\sup_{n\\ge k} K_{n}(d_{n}) \\\\\n",
    "K^{num}_{\\text{abs}}(d_{n}) &= \\lim_{k\\rightarrow \\infty} \\sup_{n\\ge k} K_{\\text{abs},n}(d_{n})\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stability of Numerical Methods\n",
    "\n",
    "The numerical method is said to be **well conditioned** if $K^{num}$ is \"small\" for any admissible datum $d_{n}$; **ill-conditioned** otherwise.\n",
    "\n",
    "A more formal way of defining the convergence of a numerical method is as follows:\n",
    "A numerical method \n",
    "\\begin{equation}\n",
    "F_{n}(x_{n}, d_{n})=0 \\quad n \\ge 1\n",
    "\\end{equation}\n",
    "is **convergent** if and only if\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "&^{\\forall}\\epsilon > 0, \\quad ^{\\exists}n_{0}(\\epsilon), \\quad ^{\\exists}\\delta(n_{0},\\epsilon) > 0 \\quad \\text{such that} \\\\\n",
    "&^{\\forall}n \\gt n_{0}(\\epsilon), \\quad ^{\\forall}\\Vert \\delta d_{n} \\Vert < \\delta(n_{0},\\epsilon), \\quad\n",
    "\\Vert x(d) - x_{n}(d+\\delta d_{n}) \\Vert \\le \\epsilon.\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What would matter more to us is how to measure the convergence, in other words, how to measure the error of an approximate solution.\n",
    "\n",
    "- **Absolute error**: $E(x_{n}) = \\Vert x - x_{n} \\Vert $\n",
    "- **Releative error**: $E_{rel}(x_{n}) = \\Vert x-x_{n} \\Vert / \\Vert x \\Vert $ if $\\Vert x \\Vert \\neq 0 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources of Error\n",
    "\n",
    "- A physical problem (PP), of which solution is denoted as $x_{ph}$\n",
    "- $F(x,d)=0$: A mathematical model of (PP)\n",
    "- $F_{n}(\\hat{x}_{n},d_{n})=0$: A computational model for PP\n",
    "\n",
    "The error associated with the computational model ($e$) is\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "e &= e_{m} + e_{c} \\\\\n",
    "  &= (x-x_{ph}) + (\\hat{x}_{n}-x),\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where \n",
    "\n",
    "- $e_{m}$: Error of mathematical model $+$ error in data\n",
    "- $e_{c}$: Discretization error ($e_{n}$) $+$ numerical algorithm $+$ roundoff error ($e_{a}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sources of Error\n",
    "\n",
    "With the above the notations, we can summarize the sources of errors as in the next figure from Quarteroni (2000):\n",
    "\n",
    "<img src=\"./Figures/Quarteroni_Fig2.1.PNG\" width=480 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other criteria for good numerical methods\n",
    "Of course, convergence is of the prime importance for a numerical method, the following concepts are also considered when choosing or developing a numerical method:\n",
    "\n",
    "- **Accuracy**: $e$ is small with respect to a fixed tolerance. \n",
    "    - Usually quantified as $e_{n}$ with respect to the discretization characteristic parameter\n",
    "    - e.g., the largest grid spacing between the discretization nodese.g., $e_{n} \\sim h^{2}$.\n",
    "- **Reliability**: $e$ is likely to be below a certain tolerance. Needs testing (benchmarking).\n",
    "- **Efficiency**: The computational complexity needed to control the error is as small as possible.\n",
    "    - **Complexity of an algorithm**: A measure of execution time\n",
    "    - **Complexity of a problem**: The complexity of the most efficient among the algorithms for the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine representation of numbers\n",
    "\n",
    "### The *positional representation* of a real number\n",
    "\n",
    "\\begin{equation}\n",
    "x_{\\beta} = (-1)^{s} \\left[ x_{n}\\, x_{n-1}\\, \\cdots\\, x_{1}\\, x_{0}.x_{-1}\\, x_{-2}\\,\\cdots x_{-m} \\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is *base*, $s$ determines the sign (0: $+$, 1: $-$), and the point is the decimal point if $\\beta=10$ or the binary point if $\\beta=2$.\n",
    "\n",
    "This representation can be also written as\n",
    "\\begin{equation}\n",
    "x_{\\beta} = (-1)^{s}\\left( \\sum_{k=-m}^{n} x_{k}\\beta^{k} \\right),\n",
    "\\end{equation}\n",
    "where $0\\le x_{k} \\lt \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine representation of numbers\n",
    "\n",
    "### The *positional representation* of a real number\n",
    "\n",
    "#### Example\n",
    "\n",
    "\\begin{matrix}\n",
    "x_{10} &= 425.33 = &4\\cdot 10^{2} &+ 2\\cdot 10 &+  5 &+  3\\cdot 10^{-1} &+ 3\\cdot 10^{-2} \\\\\n",
    "x_{6}  &= 425.33 = &4\\cdot 6^{2}  &+ 2\\cdot 6  &+  5 &+  3\\cdot 6^{-1}  &+ 3\\cdot 6^{-2}\n",
    "\\end{matrix}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x_{10} &= 1/3 = 0.3333\\cdots = 0.\\bar{3} \\\\\n",
    "x_{3}  &= 1/3 = 0.1\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The *fixed-point* number system\n",
    "\n",
    "Let's say we have $N$ memory positions in a computer. Then the **fixed-point system** goes as follows:\n",
    "\\begin{equation}\n",
    "x=\\underbrace{(-1)^{s}}_\\text{takes 1 space} [ \\underbrace{a_{n-2}\\, a_{n-3}\\, \\cdots\\, a_{k}}_\\text{N-1-k}.\\underbrace{a_{k-1}\\, \\cdots a_{0}}_\\text{k} ].\n",
    "\\end{equation}\n",
    "An equivalent summation expression is\n",
    "\\begin{equation}\n",
    "x=(-1)^{s} \\underbrace{\\beta^{-k}}_\\text{a fixed scaling factor} \\sum_{j=0}^{N-2} a_{j}\\beta^{j},\n",
    "\\end{equation}\n",
    "Since $k$ is a fixed number, minimum and maximum of the representable numbers are very limited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The *floating-point* number system\n",
    "\n",
    "Let's consider another number representation:\n",
    "\\begin{equation}\n",
    "x = (-1)^{s} (0.a_{1}\\, a_{2}\\, \\cdots\\, a_{t})\\beta^{e} = (-1)^{s}\\, m \\, \\beta^{e-t},\n",
    "\\end{equation}\n",
    "where $t \\in \\mathbb{N}$ is the number of allowed significant digits $a_{i}$, $m=a_{1}\\, a_{2}\\, \\cdots\\, a_{t}$ an integer number called \"mantissa\" ($0 \\le m \\le \\beta^{t}-1$) and $e$ an integer called \"exponent\" ($L\\le e \\le U$ and $L\\lt 0, U\\gt 0$).\n",
    "\n",
    "- **single precision**: N = 32 bits\n",
    "<img src=\"./Figures/Quarteroni_singleprecision.PNG\" width=400 />\n",
    "\n",
    "- **double precision**: N = 64 bits\n",
    "\n",
    "<img src=\"./Figures/Quarteroni_doubleprecision.PNG\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's denote the set of **floating-point numbers** with $t$ significant digits, base $\\beta \\ge 2$, $0 \\le a_{i} \\le \\beta-1$, and range $(L,U)$ with $L \\le e \\le U$ by\n",
    "\\begin{equation}\n",
    "\\mathbb{F}(\\beta, t, L, U) = \\{0\\} \\, \\cup \\, \\left\\{ x \\in \\mathbb{R}: x = (-1)^{s} \\beta^{e} \\sum_{i=1}^{t} a_{i}\\beta^{-i} \\right\\}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "$\\beta=10$, $t$=4, $L=-1$, $U=4$. If $a_{1}$ can be 0, we end up with the following redundancy:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "1 &= 0.1000 \\cdot 10^{1} \\\\\n",
    "  &= 0.0100 \\cdot 10^{2} \\\\\n",
    "  &= 0.0010 \\cdot 10^{3} \\\\\n",
    "  &= 0.0001 \\cdot 10^{4}\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Therefore $a_{1} \\neq 0$ for a unique representation and this representation is called *normalized*.\n",
    "Read pp 47-48 and Example 2.11 of Quarteroni (2000) for further discussion on normalized and *denormalized* representations.\n",
    "\n",
    "\n",
    "We can see immediately that \n",
    "\n",
    "- if $x \\in \\mathbb{F}(\\beta, t, L, U)$, then $-x \\in \\mathbb{F}$.\n",
    "- $x_{min} = \\beta^{L-1} \\le |x| \\le \\beta^{U}(1-\\beta^{-t})=x_{max}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example\n",
    "Find all the positivie numbers in the set $\\mathbb{F}(2,3,-1,2)$.\n",
    "\n",
    "Verify that $x_{min}=\\beta^{L-1}=2^{-2}=1/4$ and $x_{max}=\\beta^{U}(1-\\beta^{-t})=2^{2}(1-s^{-3})=7/2$.\n",
    "\n",
    "\n",
    "The *standard* floating-point numbers are \n",
    "\n",
    "- $\\mathbb{F}(2,24,-125,128)$ for the single precision\n",
    "- $\\mathbb{F}(2,53,-1021,1024)$ for the double precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Floating-point numbers and others available in Python\n",
    "From https://github.com/mitmath/18335/blob/master/notes/Floating-Point-Intro.ipynb, but converted to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 1.5e7 # a floating-point value 1.5 × 10⁷\n",
    "y = 1\n",
    "print(type(x), type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# division of two integers produces a floating-point value\n",
    "x = 1/49\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since $1/49 \\notin \\mathbb{F}$, however, $x$ is actually a *rounded* version of $1/49$, and multiplying it by $49$ will yield something that is close to but *not quite equal to 1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x * 49"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "1 - x * 49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about $1.11 \\times 10^{-16}$ or $2^{-53} = 0.5 * 2^{-52}$. Let's think about why we got this value.\n",
    "\n",
    "The default floating-point precision in Python is **double precision**, with $t=52$ bits of significand and $\\beta=2$ (forget about Quarteroni's notation $\\mathbb{F}(2,53,-1021,1024)$ for now.) So, the precision can be represented by the machine precision, $\\epsilon_\\mathrm{M} = 2^{-52}$, which is the distance betwen the number 1 and the nearest floating-point number: i.e., the smallest number of $\\mathbb{F}$ such that $1+\\epsilon_\\mathrm{M} > 1$. It **bounds** the relative error between any element of $\\mathbb{R}$ and the closest element of $\\mathbb{F}$. The maximum relative rounding error when rounding a number to the nearest representable one (the machine epsilon) is therefore half of $\\epsilon_\\mathrm{M} = 2^{−53}$.\n",
    "\n",
    "Back to Quarteroni's notation $\\mathbb{F}(2,53,-1021,1024)$. The explicitly stored significands are 52 bits, but there is one bit used for an implicit bit, of which value is assumed to be 1 (does the *normalized representation* ring a bell?). However, this bit is not stored. That's why a double precision number still occupies only 64 bits in computer memory!\n",
    "\n",
    "This implicit bit is assumed to be zero for *subnormal numbers* with all the exponent bits being zeros. These number are a subset of the *denormalized representation* . The subnormal numbers are introduced to represent smaller numbers than the smallest possible in the normalized representation and signed zeros. \n",
    "\n",
    "Double precision, called the `Float64` type in Python (64 bits overall), is used because it is **fast**: double-precision floating-point arithmetic is implemented by dedicated circuits in your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# these are all the same thing\n",
    "print(2.0**(-52), 2.0**(-53), np.finfo(np.float64).eps, np.finfo(1.0).eps)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* An error by 1 in the **last significant digit** is called a **1 [ulp](https://en.wikipedia.org/wiki/Unit_in_the_last_place)** (**u**nit in the **l**ast **p**lace) error, equivalent to a relative error of $\\epsilon_\\mathrm{machine}$.\n",
    "\n",
    "In fact, there is typically a small rounding error as soon as you enter a floating-point value, because most decimal fractions are not in $\\mathbb{F}$.   This can be seen by either:\n",
    "* converting to a higher precision with `big(x)` (converts to `BigFloat` [arbitrary-precision](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic) value, by default with $p=256 \\mathrm{bits}$ or about $77 \\approx 256 \\times \\log_{10}(2)$ decimal digits)\n",
    "* comparing to an exact rational — in Julia, `p // q` produces a `Rational` type, which is stored as a pair of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal, getcontext\n",
    "\n",
    "# Set the precision\n",
    "print(getcontext())\n",
    "getcontext().prec = 32\n",
    "\n",
    "# Create Decimal objects: \n",
    "x = Decimal('1')\n",
    "y = Decimal('49')\n",
    "\n",
    "# Perform calculations\n",
    "result = x/y\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "49 * (x/y) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from fractions import Fraction\n",
    "\n",
    "x = Fraction(3,2)\n",
    "print(x, type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# dump lets us see how the underlying data of Rational is stored, as 2 integers\n",
    "print( x.as_integer_ratio(), x.numerator, x.denominator )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1.5 is exactly represented in binary floating point:\n",
    "print(Decimal(1.5) == Fraction(3,2), 1.5 == Fraction(3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( 1/49 == Fraction(1, 49) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 0.1 is *not* exactly represented\n",
    "Decimal(0.1), Fraction(1,10), 0.1 == Fraction(1,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that when you enter a floating-point literal like `0.1` in Python, it is immediately converted to the nearest `Float64` value.   So `Decimal(0.1)` *first* rounds `0.1` to `Float64` and *then* converts to `Decimal`.\n",
    "\n",
    "If, instead, you want to round `0.1` to the nearest `BigFloat` directly, you have to use a different \"string macro\" input format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( Decimal(1.0e-1), Decimal((0, (1,), -1)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1e10 in 𝔽 for Float64 (about 15 decimal digits)\n",
    "Decimal(1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# 1e100 is also *not* exactly represented in Float64 precision,\n",
    "# since it not a \"small\" (≈15 digit) integer times a power of two,\n",
    "# but *is* exactly represented in 256-bit BigFloat:\n",
    "print(Decimal(1e100)) # rounds 1e100 to Float64 then extends to BigFloat\n",
    "print(Decimal(1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e100))\n",
    "print(Decimal(10.0)**Decimal(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "getcontext().prec = 256\n",
    "print(Decimal(1e100)) # rounds 1e100 to Float64 then extends to BigFloat\n",
    "print(Decimal(1.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000e100))\n",
    "print(Decimal(10.0)**Decimal(100))\n",
    "print(Decimal( (0,(1,),100) )) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Non-associativity:\n",
    "\n",
    "In particular, note that floating-point arithmetic is **not associative**.\n",
    "Try to guess the values you will get for these calculations:\n",
    "\n",
    "- `(1.0 + -1.0) + 1e-100`\n",
    "- `1.0 + (-1.0 + 1e-100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(1.0 + -1.0) + 1e-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "1.0 + (-1.0 + 1e-100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is an example of **catastrophic cancellation**: we lost *all* the significant digits.  We'll talk more about this below.\n",
    "\n",
    "Even 256 bits of precision (77 decimal digits) is not enough to avoid catastrophic cancellation here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "getcontext().prec = 64\n",
    "print( Decimal(1e-100) )\n",
    "print( Decimal(1.0) + (Decimal(-1.0) + Decimal(1e-100)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This happens because $-1.0 \\oplus \\operatorname{fl}(10^{-100}) = -1.0$ in double precision — we only have about 15 decimal places of precision, so the exact result is rounded to $-1.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "getcontext().prec = 256\n",
    "print( Decimal(1e-100) )\n",
    "print( Decimal(1.0) + (Decimal(-1.0) + Decimal(1e-100)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overflow, Underflow, Inf, and NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because a floating-point value uses a finite number of bits to store the exponent `e`, there is a maximum and minimum magnitude for floating-point values.   If you go over the maximum, you **overflow** to a special `Inf` value (or `-Inf` for large negative values), representing $\\infty$.   If you go under the minimum, you **underflow** to $\\pm 0.0$, where $-0$ is used to represent e.g. a value that underflowed from the negative side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "1e300 # okay: 10³⁰⁰ is in the representable range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(1e300)**2 # overflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "-np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "1 / np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get the maximum representable magnitude via `floatmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# For float64 (double precision)\n",
    "max_float64 = np.finfo(np.float64).max\n",
    "print(max_float64)\n",
    "\n",
    "# For float32 (single precision)\n",
    "max_float32 = np.finfo(np.float32).max\n",
    "print(max_float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "1e-300 # okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(1e-300)**2 # underflows to +0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# For float64 (double precision)\n",
    "print(np.finfo(np.float64).min)\n",
    "print(np.finfo(np.float32).min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "-1e-300 * 1e-300 # underflows to -0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While $-0$ is printed differently from $+0$, they still compare equal.  However, you will notice the difference if you do something that depends on the sign:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "+0.0 == -0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dividing by zero gives `Inf`, as you expect, or `-Inf` if you divide by \"negative zero\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(1 / +0.0, 1 / -0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( np.divide(1, +0.0), np.divide(1, -0.0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 1/-Inf is -0.0, this has the nice property that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( 1 / (1 / -np.inf) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( np.divide(1, np.divide( 1, -np.inf)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A special value `NaN` (\"not a number\") is used to represent the result of floating-point operations that can't be defined in a sensible way (e.g. [indeterminate forms](https://en.wikipedia.org/wiki/Indeterminate_form)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( 0 * np.inf, np.inf / np.inf, np.divide(0,0), 0 * np.nan )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, **non-finite** values are the exception to the rule that $0 \\otimes x == 0$ in floating-point arithmetic.\n",
    "\n",
    "In fact, `NaN` has the odd property that it is the *only* number that is not equal to itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( np.nan == np.nan )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One way of viewing IEEE's semantics is that a `NaN` can be viewed as a stand-in for *any* value, or *none*, so `NaN` values arising from different sources are not equivalent.  (In some statistical software, `NaN` is also used to represent missing data, but Julia has a special [`missing` value](https://docs.julialang.org/en/v1/manual/missing/) for this.)\n",
    "\n",
    "You can check for non-finite values like this with `isnan`, `isinf`, and `isfinite`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( np.isinf(2.5), np.isinf(np.inf), np.isinf(-np.inf), np.isinf(np.nan) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.isnan(2.5), np.isnan(np.inf), np.isnan(-np.inf), np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print( np.isfinite(2.5), np.isfinite(np.inf), np.isfinite(-np.inf), np.isfinite(np.nan) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In some other languages, `NaN` is also used to signal that a function cannot be evaluated. For example, in Numpy, `sqrt(-1.0)` returns `NaN`. The `sqrt` in Python's math library, however, throws an exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(np.sqrt(-1.0))\n",
    "print(math.sqrt(-1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cancellation error\n",
    "\n",
    "One common source of huge floating-point errors is a [catastrophic cancellation](https://en.wikipedia.org/wiki/Loss_of_significance): if you **subtract two nearly equal numbers** then most of the significant digits cancel, and the result can have a relative error $\\gg \\epsilon$.\n",
    "\n",
    "Catastrophic cancellation is not inevitable, however!  Often it can be avoided simply by **re-arranging your calculation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The `expm1` function\n",
    "\n",
    "Suppose you are calculating the function $e^x - 1$ using floating-point arithmetic.   When $|x| \\ll 1$, we have $e^x \\approx 1$, and so a naive calculation $e^x \\ominus 1$ will experience catastrophic cancellation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = 2.0**(-60)\n",
    "print(x)\n",
    "print(np.exp(x))\n",
    "print(np.exp(x) - 1) # naive algorithm: catastrophic cancellation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This result `0.0` has **no correct digits**.  The correct answer is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# naive algorithm computed in BigFloat precision and rounded back to Float64:\n",
    "print( np.exp(Decimal(x)) - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can also see this using the Taylor expansion of $e^x$:\n",
    "\n",
    "$$\n",
    "e^x - 1 = \\left(1 + x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!} + \\cdots\\right) - 1 = \\boxed{x + \\frac{x^2}{2} + \\cdots + \\frac{x^n}{n!} + \\cdots}\n",
    "$$\n",
    "which we can use to calculate this function accurately for small $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x + x**2/2 + x**3/6 # 3 terms is more than enough for x ≈ 8.7e-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x # in fact, just one term is enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key is to **rearrange the calculation** to **perform the cancellation analytically**, and only use floating-point arithmetic *after* this is accomplished.\n",
    "\n",
    "In fact, Numpy library (and scientific-computing libraries in other languages) provides a function called `expm1(x)` that computes $e^x - 1$ accurately for all `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.expm1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such [special functions](https://en.wikipedia.org/wiki/Special_functions) can be implemented in many ways.  One possible implementation of `expm1` might be:\n",
    "\n",
    "* Just do `exp(x) - 1` if $|x|$ is sufficiently large.\n",
    "* Use the Taylor series if $|x|$ is small.\n",
    "* In between (e.g. $|x| \\sim 1$), to avoid requiring many terms of the Taylor series, one could use some kind of fit, e.g. a [minimax polynomial](https://en.wikipedia.org/wiki/Minimax_approximation_algorithm) or [rational function](https://en.wikipedia.org/wiki/Rational_function).\n",
    "\n",
    "(In general, special-function implementations typically use some combination of Taylor series near zeros, minimax fits, continued-fraction expansions or asymptotic series, and function-specific identities.  This is a branch of numerical analysis that we won't delve into in 18.335.)\n",
    "\n",
    "Sometimes, a simple (but often non-obvious) algebraic rearrangement leads to a formula that is accurate for all $x$.  For example, in this case one can use the exact identities:\n",
    "$$\n",
    "e^x - 1 = \\left(e^x+1\\right)\\tanh(x/2) = \\frac{\\left(e^x - 1\\right) x}{\\log\\left(e^x\\right)}\n",
    "$$\n",
    "and it turns out that the catastrophic cancellation is avoided with either of the two expressions at right, at the cost of calling `tanh` or `log` in addition to `exp`.  See e.g. Higham, [*Accuracy and Stability of Numerical Algorithms*](https://epubs.siam.org/doi/book/10.1137/1.9780898718027?mobileUi=0) (2002), p. 30 for more explanation and references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quadratic roots\n",
    "\n",
    "If you are finding solutions of the quadratic equation\n",
    "$$\n",
    "ax^2 + bx + c = 0\n",
    "$$\n",
    "you will surely reach for the [quadratic formula](https://en.wikipedia.org/wiki/Quadratic_formula):\n",
    "$$\n",
    "x_\\pm = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n",
    "$$\n",
    "However, suppose $b > 0$ and $|ac| \\ll b^2$.   In this case, $\\sqrt{b^2 - 4ac} \\approx b$.  The $x_-$ root will be fine, but the $x_+$ root will suffer from a catastrophic cancellation because $-b + \\sqrt{\\cdots}$ is the difference of two nearly equal quantities.\n",
    "\n",
    "To compute $x_+$, we could again use a Taylor series, but it turns out that we can instead use a simple re-arrangement:\n",
    "$$\n",
    "x_\\pm = \\frac{2c}{-b \\mp \\sqrt{b^2 - 4ac}}\n",
    "$$\n",
    "which comes from dividing our quadratic equation by $x^2$ and applying the standard quadratic formula to $cy^2 + by + a = 0$ where $y = 1/x$.   This \"inverted\" form of the quadratic formula is accurate for $x_+$ (again assuming $b > 0$) but may have catastrophic cancellation for $x_-$.\n",
    "\n",
    "So, we just use the first quadratic formula for the $x_-$ root and the second \"inverted\" quadratic formula for the $x_+$ root:\n",
    "$$\n",
    "x_+, \\, x_- = \\frac{2c}{-b - \\sqrt{b^2 - 4ac}},\\;\\frac{-b - \\sqrt{b^2 - 4ac}}{2a} \\, .\n",
    "$$\n",
    "No increase in computational cost, just a little thought and rearrangement."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
